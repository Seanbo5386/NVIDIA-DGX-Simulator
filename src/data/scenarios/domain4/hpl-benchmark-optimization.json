{
  "id": "domain4-hpl-optimization",
  "title": "HPL Benchmark Optimization and Performance Tuning",
  "domain": "domain4",
  "difficulty": "advanced",
  "description": "Master the art of optimizing High-Performance LINPACK (HPL) benchmarks on DGX systems. HPL is the industry standard for measuring floating-point computational performance and is used to rank systems on the TOP500 list. This scenario covers configuration tuning, performance analysis, and achieving optimal FLOPS.",
  "learningObjectives": [
    "Understand HPL benchmark parameters and their impact",
    "Configure HPL.dat for DGX hardware",
    "Analyze HPL output and identify bottlenecks",
    "Tune for maximum GFLOPS performance",
    "Validate results against expected baselines"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Verify System Readiness for HPL",
      "description": "Before running HPL, verify the system is properly configured for maximum performance including GPU clocks, ECC mode, and persistence mode.",
      "objectives": [
        "Check GPU persistence mode",
        "Verify GPU clock settings",
        "Check ECC mode status",
        "Ensure no thermal throttling"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d CLOCK",
        "nvidia-smi -q -d PERFORMANCE",
        "nvidia-smi -pm 1",
        "nvidia-smi -q -d TEMPERATURE"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify system readiness",
          "expectedCommands": ["nvidia-smi -q -d CLOCK", "nvidia-smi -pm"]
        }
      ],
      "hints": [
        "Persistence mode reduces initialization overhead",
        "Maximum clocks: nvidia-smi -ac <mem_clock>,<gpu_clock>",
        "ECC can be disabled for max performance (not recommended for production)",
        "Temperature should be well below throttle threshold",
        "Check power limit is at maximum: nvidia-smi -pl <watts>"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVIDIA HPL Guide",
          "url": "https://docs.nvidia.com/hpc-benchmarks/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Configure HPL.dat Parameters",
      "description": "Configure the HPL input file (HPL.dat) with optimal parameters for the DGX system's memory and GPU count.",
      "objectives": [
        "Calculate optimal problem size (N)",
        "Set block size (NB)",
        "Configure process grid (P x Q)",
        "Set algorithmic parameters"
      ],
      "expectedCommands": [
        "cat HPL.dat",
        "nvidia-smi --query-gpu=memory.total --format=csv",
        "free -g"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must examine HPL configuration",
          "expectedCommands": ["cat HPL.dat", "nvidia-smi --query-gpu"]
        }
      ],
      "hints": [
        "N (problem size): ~90% of total GPU memory in double-precision elements",
        "Formula: N = sqrt(0.9 * total_gpu_memory / 8)",
        "NB (block size): 512 or 1024 optimal for A100/H100",
        "P x Q = number of GPUs (e.g., 2x4 or 4x2 for 8 GPUs)",
        "PFACT and RFACT: 2 (Crout) usually optimal"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "HPL Tuning Guide",
          "url": "https://www.netlib.org/benchmark/hpl/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Run Initial HPL Benchmark",
      "description": "Execute the HPL benchmark with baseline configuration and capture initial performance metrics.",
      "objectives": [
        "Launch HPL with NGC container",
        "Monitor GPU utilization during run",
        "Capture GFLOPS result",
        "Note execution time"
      ],
      "expectedCommands": [
        "docker run --gpus all -v $(pwd):/workspace nvcr.io/nvidia/hpc-benchmarks:23.10 ./hpl.sh",
        "nvidia-smi dmon -s pucvmet -d 5",
        "nvidia-smi --query-gpu=utilization.gpu,power.draw --format=csv -l 5"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run HPL benchmark",
          "expectedCommands": ["docker run", "nvidia-smi dmon"]
        }
      ],
      "hints": [
        "NGC container includes optimized HPL binary",
        "Monitor GPU utilization - should be near 100%",
        "Low utilization indicates configuration issue",
        "Expected A100 single-node: ~24 TFLOPS",
        "Expected H100 single-node: ~50+ TFLOPS"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "NGC HPL Container",
          "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/containers/hpc-benchmarks"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Analyze HPL Output",
      "description": "Parse and analyze the HPL output to understand performance characteristics and identify optimization opportunities.",
      "objectives": [
        "Read HPL output summary",
        "Identify GFLOPS achieved",
        "Calculate efficiency percentage",
        "Check for WR (wrong result) flags"
      ],
      "expectedCommands": [
        "grep -E 'WR|Gflops' hpl_output.txt",
        "tail -20 hpl_output.txt"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must analyze HPL output",
          "expectedCommands": ["grep", "hpl_output"]
        }
      ],
      "hints": [
        "Look for 'WR' column - should all show PASSED",
        "GFLOPS in rightmost column of result line",
        "Efficiency = Achieved GFLOPS / Theoretical Peak GFLOPS",
        "A100 theoretical peak: 312 TFLOPS (TF32)",
        "Good efficiency: >80% of theoretical peak"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "HPL Output Interpretation",
          "url": "https://www.netlib.org/benchmark/hpl/tuning.html"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Tune Problem Size and Block Size",
      "description": "Iteratively tune N (problem size) and NB (block size) to find optimal configuration.",
      "objectives": [
        "Test multiple problem sizes",
        "Try different block sizes (256, 512, 1024)",
        "Record performance for each configuration",
        "Find optimal N/NB combination"
      ],
      "expectedCommands": [
        "sed -i 's/^[0-9]* *Ns$/120000 Ns/' HPL.dat",
        "sed -i 's/^[0-9]* *NBs$/1024 NBs/' HPL.dat",
        "cat HPL.dat | head -10"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must tune HPL parameters",
          "expectedCommands": ["sed", "HPL.dat"]
        }
      ],
      "hints": [
        "Larger N = higher GFLOPS but longer runtime",
        "N too large = out of memory error",
        "NB should divide N evenly for best performance",
        "Common NB values: 256, 384, 512, 768, 1024",
        "A100 80GB optimal N: ~140,000-160,000"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "HPL Parameter Tuning",
          "url": "https://www.netlib.org/benchmark/hpl/tuning.html"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Optimize Process Grid",
      "description": "Optimize the process grid (P x Q) for the GPU topology and NVLink configuration.",
      "objectives": [
        "Understand GPU topology impact",
        "Test different P x Q configurations",
        "Align process grid with NVLink topology",
        "Minimize communication overhead"
      ],
      "expectedCommands": [
        "nvidia-smi topo -m",
        "sed -i 's/^[0-9]* *Ps$/2 Ps/' HPL.dat",
        "sed -i 's/^[0-9]* *Qs$/4 Qs/' HPL.dat"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must optimize process grid",
          "expectedCommands": ["nvidia-smi topo", "Ps", "Qs"]
        }
      ],
      "hints": [
        "P x Q must equal number of GPUs",
        "Q >= P is generally better (more column processes)",
        "Align with NVLink domains when possible",
        "For 8 GPUs: try 2x4, 4x2, 1x8, 8x1",
        "NVSwitch systems: topology less critical"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "GPU Topology and HPL",
          "url": "https://docs.nvidia.com/hpc-benchmarks/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Validate and Document Results",
      "description": "Run final optimized configuration, validate results, and document performance baseline.",
      "objectives": [
        "Run final optimized HPL",
        "Verify all tests PASSED",
        "Calculate final efficiency",
        "Document optimal configuration"
      ],
      "expectedCommands": [
        "docker run --gpus all -v $(pwd):/workspace nvcr.io/nvidia/hpc-benchmarks:23.10 ./hpl.sh > hpl_final.txt 2>&1",
        "grep -E 'WR|Gflops' hpl_final.txt",
        "cp HPL.dat HPL.dat.optimized"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must validate final results",
          "expectedCommands": ["docker run", "grep", "optimized"]
        }
      ],
      "hints": [
        "All WR columns must show PASSED for valid result",
        "Save optimized HPL.dat for future use",
        "Document: N, NB, P, Q, GFLOPS, efficiency",
        "Compare against NVIDIA published baselines",
        "Run multiple times to ensure consistency"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "DGX HPL Baselines",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Verified system readiness for benchmarking",
    "Understood HPL.dat parameters",
    "Ran baseline HPL benchmark",
    "Analyzed HPL output correctly",
    "Tuned problem size and block size",
    "Optimized process grid for topology",
    "Achieved performance improvement over baseline"
  ],
  "estimatedTime": 55,
  "prerequisites": [
    "domain4-hpl-benchmark-workflow"
  ],
  "tags": [
    "hpl",
    "linpack",
    "benchmark",
    "performance",
    "tuning",
    "advanced",
    "domain4",
    "exam-critical"
  ]
}
