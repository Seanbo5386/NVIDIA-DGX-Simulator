{
  "id": "domain4-perf-baseline",
  "title": "Establishing Performance Baselines",
  "domain": "domain4",
  "difficulty": "intermediate",
  "description": "Learn how to establish comprehensive performance baselines for DGX systems. Baselines are essential for acceptance testing, ongoing monitoring, and troubleshooting.",
  "learningObjectives": [
    "Define performance metrics to baseline",
    "Run standardized benchmarks",
    "Document baseline measurements",
    "Create ongoing monitoring plan",
    "Detect performance regressions"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Define Baseline Metrics",
      "description": "Identify the key performance metrics to measure and track.",
      "objectives": [
        "List compute metrics",
        "List memory metrics",
        "List interconnect metrics"
      ],
      "expectedCommands": [
        "nvidia-smi -q | head -100",
        "nvidia-smi --query-gpu=name,memory.total,power.limit --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must define baseline metrics",
          "expectedCommands": ["nvidia-smi -q", "nvidia-smi --query-gpu"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Compute: TFLOPS (HPL), GPU utilization",
        "Memory: Bandwidth (GB/s), capacity",
        "Interconnect: NVLink BW, IB BW",
        "Storage: Read/write throughput, IOPS",
        "System: Power draw, temperatures"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Performance Metrics",
          "url": "https://docs.nvidia.com/datacenter/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Capture System Configuration",
      "description": "Document the hardware and software configuration for baseline context.",
      "objectives": [
        "Record hardware specs",
        "Document software versions",
        "Note configuration settings"
      ],
      "expectedCommands": [
        "nvidia-smi -q | head -50",
        "cat /proc/driver/nvidia/version",
        "uname -a"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must capture system configuration",
          "expectedCommands": ["nvidia-smi -q", "cat /proc/driver/nvidia/version", "uname -a"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "GPU model, count, VBIOS version",
        "Driver and CUDA version",
        "OS kernel version",
        "Network configuration",
        "Power limits and clock settings"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "System Documentation",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Baseline GPU Memory Bandwidth",
      "description": "Measure GPU memory bandwidth using standard tests.",
      "objectives": [
        "Run memory bandwidth test",
        "Record peak bandwidth",
        "Compare to theoretical"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=memory.total,memory.free --format=csv",
        "nvidia-smi -q -d MEMORY"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must baseline memory bandwidth",
          "expectedCommands": ["nvidia-smi --query-gpu=memory.total", "nvidia-smi -q -d MEMORY"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "A100: ~2 TB/s HBM2e bandwidth",
        "H100: ~3.35 TB/s HBM3 bandwidth",
        "Run CUDA bandwidthTest sample",
        "Measure host-to-device and device-to-device",
        "Results should be 80%+ of theoretical"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Memory",
          "url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Baseline NVLink Bandwidth",
      "description": "Measure GPU-to-GPU NVLink bandwidth.",
      "objectives": [
        "Check NVLink topology",
        "Run P2P bandwidth test",
        "Record bidirectional bandwidth"
      ],
      "expectedCommands": [
        "nvidia-smi topo -m",
        "nvidia-smi nvlink --status",
        "nvidia-smi nvlink -c"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must baseline NVLink bandwidth",
          "expectedCommands": ["nvidia-smi topo -m", "nvidia-smi nvlink --status"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "A100: ~600 GB/s bidirectional",
        "H100: ~900 GB/s bidirectional",
        "Run p2pBandwidthLatencyTest from CUDA samples",
        "Measure all GPU pairs",
        "Verify full NVLink connectivity"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "NVLink Bandwidth",
          "url": "https://docs.nvidia.com/datacenter/nvlink/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Baseline NCCL Collective Performance",
      "description": "Measure collective communication performance with NCCL tests.",
      "objectives": [
        "Run all_reduce test",
        "Record bandwidth vs message size",
        "Document latency"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "nvidia-smi topo -m"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must baseline NCCL performance",
          "expectedCommands": ["nvidia-smi", "nvidia-smi topo -m"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Run nccl-tests all_reduce_perf",
        "Measure across message size range",
        "Expected algBW > 200 GB/s at 1GB",
        "Document both algBW and busBW",
        "Test single and multi-node"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NCCL Performance",
          "url": "https://docs.nvidia.com/deeplearning/nccl/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Baseline Storage Performance",
      "description": "Measure storage subsystem performance.",
      "objectives": [
        "Test sequential throughput",
        "Test random IOPS",
        "Document filesystem performance"
      ],
      "expectedCommands": [
        "df -h",
        "mount | grep -E 'lustre|gpfs|nfs'"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must baseline storage performance",
          "expectedCommands": ["df -h", "mount | grep"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use fio for comprehensive I/O testing",
        "Test both local and shared storage",
        "Sequential read/write throughput",
        "Random read/write IOPS",
        "Test at various queue depths"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Storage Testing",
          "url": "https://linux.die.net/man/1/fio"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Create Baseline Document",
      "description": "Compile all measurements into a formal baseline document.",
      "objectives": [
        "Format measurements",
        "Include configuration context",
        "Set acceptable ranges"
      ],
      "expectedCommands": [
        "nvidia-smi -q > baseline_gpu_info.txt",
        "nvidia-smi topo -m > baseline_topology.txt"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must create baseline document",
          "expectedCommands": ["nvidia-smi -q >", "nvidia-smi topo -m >"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Include test date and conditions",
        "Document all software versions",
        "Set Â±5% acceptable range",
        "Store in version control",
        "Update after upgrades"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "Baseline Documentation",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step8",
      "title": "Set Up Ongoing Monitoring",
      "description": "Configure automated performance monitoring and alerting.",
      "objectives": [
        "Enable DCGM monitoring",
        "Set performance thresholds",
        "Configure alerting"
      ],
      "expectedCommands": [
        "dcgmi discovery -l",
        "dcgmi stats --enable",
        "dcgmi health --check"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must set up monitoring",
          "expectedCommands": ["dcgmi discovery -l", "dcgmi stats --enable", "dcgmi health --check"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "DCGM provides continuous monitoring",
        "Set alerts for deviation from baseline",
        "Monitor temperatures and power",
        "Track ECC errors over time",
        "Regular validation tests recommended"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "DCGM Monitoring",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Defined baseline metrics",
    "Captured system configuration",
    "Baselined GPU memory bandwidth",
    "Baselined NVLink bandwidth",
    "Baselined NCCL performance",
    "Baselined storage performance",
    "Created baseline document",
    "Set up ongoing monitoring"
  ],
  "estimatedTime": 57,
  "prerequisites": ["domain1-driver-install"],
  "tags": ["baseline", "performance", "monitoring", "validation", "intermediate"]
}
