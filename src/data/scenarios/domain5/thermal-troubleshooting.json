{
  "id": "domain5-thermal",
  "title": "GPU Thermal Issue Troubleshooting",
  "domain": "domain5",
  "difficulty": "intermediate",
  "description": "Learn how to identify, diagnose, and resolve GPU thermal issues. Proper thermal management is critical for GPU performance, reliability, and longevity in datacenter environments.",
  "learningObjectives": [
    "Monitor GPU temperatures and thermal status",
    "Identify thermal throttling conditions",
    "Diagnose cooling system issues",
    "Implement thermal mitigation strategies",
    "Understand thermal limits and specifications"
  ],
  "faults": [
    {
      "nodeId": "dgx-00",
      "gpuId": 2,
      "type": "thermal",
      "severity": "warning",
      "parameters": {
        "targetTemp": 95
      }
    }
  ],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Monitor Current GPU Temperatures",
      "description": "Check current GPU temperatures across all GPUs to identify any that are running hot or approaching thermal limits.",
      "objectives": [
        "Query GPU temperatures with nvidia-smi",
        "Identify GPUs with elevated temperatures",
        "Check temperature trends over time",
        "Compare against normal operating ranges"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=index,temperature.gpu,temperature.memory --format=csv",
        "nvidia-smi dmon -s t"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must monitor GPU temperatures",
          "expectedCommands": [
            "nvidia-smi --query-gpu=temperature",
            "nvidia-smi dmon"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi --query-gpu=temperature.gpu --format=csv' for temps",
        "H100 normal operating range: 30-80°C (idle to loaded)",
        "Thermal throttling typically starts at 85-90°C",
        "Maximum safe temperature: 90-95°C (varies by GPU model)",
        "Use 'nvidia-smi dmon -s t' for continuous temperature monitoring",
        "Memory junction temps are also important (temperature.memory)",
        "Use 'watch -n 1 nvidia-smi' for real-time monitoring"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Thermal Specifications",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Check for Thermal Throttling",
      "description": "Determine if any GPUs are thermally throttling, which reduces performance to manage temperature.",
      "objectives": [
        "Query GPU clocks and thermal status",
        "Identify active throttling",
        "Check throttle reasons",
        "Measure performance impact"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=index,clocks.current.graphics,clocks.max.graphics,clocks_throttle_reasons.active --format=csv",
        "nvidia-smi -q | grep -i throttle"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check throttling status",
          "expectedCommands": [
            "nvidia-smi --query-gpu=clocks",
            "nvidia-smi -q | grep -i throttle"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use --query-gpu=clocks_throttle_reasons.active to see why GPU is throttling",
        "Throttle reasons include: thermal, power, sw_thermal, hw_slowdown",
        "'clocks_throttle_reasons.sw_thermal' = Software-imposed thermal throttling",
        "'clocks_throttle_reasons.hw_slowdown' = Hardware thermal protection",
        "Compare current clocks vs max clocks - large gap indicates throttling",
        "Thermal throttling significantly impacts training/inference performance",
        "None' throttle reasons = GPU operating normally"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "GPU Throttling",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Check Cooling System Status",
      "description": "Verify that the cooling system (fans, liquid cooling) is functioning properly and providing adequate airflow.",
      "objectives": [
        "Check fan speeds across all GPUs",
        "Verify fans are operational",
        "Check for fan failures",
        "Review BMC cooling alerts"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=index,fan.speed --format=csv",
        "ipmitool sensor list | grep -i fan"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check cooling system",
          "expectedCommands": [
            "nvidia-smi --query-gpu=fan.speed",
            "ipmitool sensor list | grep -i fan"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi --query-gpu=fan.speed' to check GPU fans",
        "Normal fan speed range: 30-80% depending on load",
        "Fan speed 0% or N/A may indicate fan failure or liquid cooling",
        "DGX systems use both GPU fans and chassis fans",
        "Use 'ipmitool sensor list | grep -i fan' for all fan sensors",
        "Check BMC for fan failure alerts",
        "Low or zero fan speed with high temps indicates cooling failure"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "DGX Cooling Systems",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Review Environmental Conditions",
      "description": "Check datacenter environmental conditions including ambient temperature, airflow, and rack configuration.",
      "objectives": [
        "Check ambient temperature via BMC",
        "Verify proper airflow direction",
        "Check for obstructions",
        "Review rack power/cooling capacity"
      ],
      "expectedCommands": [
        "ipmitool sensor list | grep -i temp",
        "ipmitool sensor list | grep -i ambient"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check environmental conditions",
          "expectedCommands": [
            "ipmitool sensor list | grep -i temp",
            "ipmitool sensor list"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use BMC sensors to check inlet/ambient temperature",
        "Recommended datacenter ambient: 18-27°C (64-80°F)",
        "Check 'Inlet Temp' or 'Ambient' sensors via ipmitool",
        "Verify cold aisle/hot aisle configuration",
        "Check for blocked vents or cable management issues",
        "Ensure rack has adequate cooling capacity for heat load",
        "High ambient temps (>30°C) can cause GPU thermal issues"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "Datacenter Environmental Requirements",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Check GPU Utilization and Workload",
      "description": "Verify that thermal issues aren't caused by abnormal workloads or stuck processes.",
      "objectives": [
        "Check GPU utilization",
        "Identify running processes",
        "Verify workload is normal",
        "Look for stuck processes"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=index,utilization.gpu,utilization.memory --format=csv",
        "nvidia-smi pmon"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check GPU workload",
          "expectedCommands": [
            "nvidia-smi --query-gpu=utilization",
            "nvidia-smi pmon"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi' to see GPU utilization and processes",
        "Use 'nvidia-smi pmon' for per-process monitoring",
        "100% utilization with high temps is normal under load",
        "High temps at idle (0% util) indicates cooling problem",
        "Check for stuck processes that keep GPU loaded",
        "Some workloads (like FP64) generate more heat than others",
        "Kill stuck processes with 'sudo kill <pid>' if needed"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Workload Monitoring",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Review Thermal Events in Logs",
      "description": "Check system logs for thermal-related events, warnings, or errors that provide clues to the thermal issue.",
      "objectives": [
        "Review dmesg for thermal events",
        "Check nvidia-smi logs",
        "Review BMC System Event Log",
        "Identify event timeline"
      ],
      "expectedCommands": [
        "dmesg | grep -i thermal",
        "dmesg | grep -i temperature",
        "ipmitool sel list | grep -i temp"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must review thermal logs",
          "expectedCommands": ["dmesg | grep -i thermal", "ipmitool sel list"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dmesg | grep -i thermal' for kernel thermal messages",
        "Look for 'thermal throttling' messages",
        "Check BMC SEL: 'ipmitool sel list | grep -i temp'",
        "BMC may log 'Upper Critical' temp threshold violations",
        "Correlate thermal events with workload changes",
        "Use 'dmesg -T' for human-readable timestamps",
        "Check DCGM logs if using DCGM monitoring"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "System Logging",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Implement Thermal Mitigation",
      "description": "Take corrective actions to resolve the thermal issue and return GPU temperatures to normal operating range.",
      "objectives": [
        "Reduce workload if necessary",
        "Improve airflow/cooling",
        "Clean dust/debris if applicable",
        "Adjust power limits if needed"
      ],
      "expectedCommands": [
        "nvidia-smi -pl <power_limit> -i <gpu_id>",
        "nvidia-smi --query-gpu=power.limit,power.default_limit --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must understand mitigation options",
          "expectedCommands": [
            "nvidia-smi -pl",
            "nvidia-smi --query-gpu=power"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Short-term: Reduce GPU power limit to lower temps",
        "Example: nvidia-smi -pl 400 -i 0 (sets 400W limit on GPU 0)",
        "Use -i <gpu_id> to target specific GPU with thermal issues",
        "Check default limit: nvidia-smi --query-gpu=power.default_limit",
        "Verify improved airflow - remove obstructions",
        "Check/replace failed fans",
        "Improve datacenter cooling if ambient temp high",
        "For persistent issues, schedule maintenance window",
        "Consider workload distribution across more GPUs",
        "Long-term: Upgrade cooling infrastructure if undersized"
      ],
      "estimatedDuration": 11,
      "documentationLinks": [
        {
          "title": "Thermal Management",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step8",
      "title": "Monitor Temperature After Mitigation",
      "description": "After implementing fixes, monitor GPU temperatures to verify the issue is resolved and stays resolved.",
      "objectives": [
        "Monitor temps for 15-30 minutes",
        "Verify temps are in normal range",
        "Confirm no throttling occurs",
        "Document resolution"
      ],
      "expectedCommands": [
        "watch -n 5 'nvidia-smi --query-gpu=index,temperature.gpu,clocks_throttle_reasons.active --format=csv'",
        "nvidia-smi dmon -s t"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must monitor after mitigation",
          "expectedCommands": [
            "nvidia-smi --query-gpu=temperature",
            "nvidia-smi dmon"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'watch -n 5 nvidia-smi' for continuous monitoring",
        "Temps should stabilize in normal range (30-80°C for H100)",
        "Verify throttle_reasons shows 'None'",
        "Monitor under typical workload, not just idle",
        "Set up alerting to detect future thermal issues",
        "Document: Root cause, actions taken, current status",
        "Schedule follow-up check after 24 hours",
        "Update maintenance records"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "Ongoing Thermal Monitoring",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Successfully monitored GPU temperatures",
    "Identified any thermal throttling",
    "Checked cooling system functionality",
    "Reviewed environmental conditions",
    "Checked GPU workload and processes",
    "Reviewed thermal event logs",
    "Implemented appropriate mitigation",
    "Verified temperature returned to normal"
  ],
  "estimatedTime": 65,
  "prerequisites": ["domain1-bmc-config", "domain1-driver-install"],
  "tags": [
    "thermal",
    "cooling",
    "temperature",
    "throttling",
    "troubleshooting",
    "intermediate"
  ],
  "tier": 2,
  "commandFamilies": ["gpu-monitoring", "bmc-hardware"],
  "explanationGateId": "gate-domain5-thermal"
}
