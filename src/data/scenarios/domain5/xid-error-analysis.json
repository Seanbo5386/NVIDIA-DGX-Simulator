{
  "id": "domain5-xid-errors",
  "title": "XID Error Analysis and Resolution",
  "domain": "domain5",
  "difficulty": "advanced",
  "description": "Learn how to identify, analyze, and resolve GPU XID errors. XID errors indicate GPU hardware or driver issues and require systematic troubleshooting to prevent workload failures.",
  "learningObjectives": [
    "Understand XID error types and severity",
    "Locate and interpret XID error messages",
    "Identify root causes of common XID errors",
    "Apply appropriate remediation steps",
    "Determine when hardware replacement is needed"
  ],
  "faults": [
    {
      "nodeId": "dgx-00",
      "gpuId": 0,
      "type": "xid-error",
      "severity": "critical",
      "parameters": {
        "xid": 79,
        "description": "GPU has fallen off the bus"
      }
    }
  ],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Identify XID Error Occurrence",
      "description": "Learn where XID errors are logged and how to identify when they occur. XID errors appear in kernel logs, nvidia-smi output, and DCGM.",
      "objectives": [
        "Check dmesg for XID error messages",
        "Query nvidia-smi for GPU errors",
        "Review DCGM error logs",
        "Identify affected GPU"
      ],
      "expectedCommands": [
        "dmesg | grep -i xid",
        "nvidia-smi -q | grep -i error",
        "dcgmi stats -v"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check for XID errors in logs",
          "expectedCommands": ["dmesg | grep -i xid", "dmesg | grep -i nvrm"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "XID errors appear in dmesg as: 'NVRM: Xid (PCI:0000:xx:xx.x): <error_number>, <description>'",
        "Use 'dmesg | grep -i xid' to find XID errors",
        "Use 'dmesg | grep -i nvrm' for all NVIDIA driver messages",
        "GPU PCI address identifies which GPU had the error",
        "Recent XID errors are more relevant than old ones",
        "Use 'dmesg -T' to see timestamps for error correlation"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "XID Error Reference",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Interpret XID Error Code",
      "description": "Understand what the specific XID error code indicates about the GPU hardware or driver issue.",
      "objectives": [
        "Look up XID error code in documentation",
        "Understand error severity (warning vs critical)",
        "Identify potential root causes",
        "Determine impact on workloads"
      ],
      "expectedCommands": ["dmesg | grep 'Xid 79'", "dmesg | grep 'Xid'"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must review XID error details",
          "expectedCommands": ["dmesg | grep 'Xid'", "dmesg | grep -i nvrm"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Common critical XIDs:",
        "- Xid 13: Graphics Engine Exception",
        "- Xid 31: GPU memory page fault",
        "- Xid 43: GPU stopped responding",
        "- Xid 48: DBE (Double Bit Error) ECC error",
        "- Xid 79: GPU fallen off the bus",
        "- Xid 94: Contained ECC error",
        "Xid 79 is one of most serious - indicates PCIe communication failure",
        "Reference full XID list: https://docs.nvidia.com/deploy/xid-errors/"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "XID Error Descriptions",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Check GPU Status After XID",
      "description": "After an XID error, verify the current state of the affected GPU and determine if it's still functional.",
      "objectives": [
        "Query GPU status with nvidia-smi",
        "Check if GPU is still visible",
        "Verify GPU can be queried",
        "Test basic GPU functionality"
      ],
      "expectedCommands": ["nvidia-smi", "nvidia-smi -q -i 0"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check GPU status",
          "expectedCommands": ["nvidia-smi", "nvidia-smi -q"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "If nvidia-smi hangs or fails, GPU may need reset",
        "Check 'Persistence Mode' - should be Enabled",
        "Check 'GPU Operation Mode' - should be Default or N/A",
        "Look for 'No devices were found' error",
        "For Xid 79 (fallen off bus), GPU typically not visible",
        "Compare GPU list before/after error",
        "Use lspci to verify GPU still on PCIe bus"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Status Checking",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Attempt GPU Reset",
      "description": "For recoverable XID errors, attempt to reset the GPU to restore functionality without a system reboot.",
      "objectives": [
        "Identify if GPU reset is appropriate",
        "Perform GPU reset",
        "Verify GPU recovery",
        "Check for recurring errors"
      ],
      "expectedCommands": ["nvidia-smi --gpu-reset -i 0", "nvidia-smi -q -i 0"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must attempt GPU recovery",
          "expectedCommands": ["nvidia-smi --gpu-reset", "nvidia-smi -q"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "GPU reset command: nvidia-smi --gpu-reset -i <gpu_id>",
        "GPU reset only works if GPU is still on PCIe bus",
        "Reset terminates any processes using the GPU",
        "For Xid 79 (fallen off bus), reset typically won't work - this is expected",
        "After reset attempt, verify GPU status with 'nvidia-smi -q -i 0'",
        "If reset succeeds, GPU should appear normal in nvidia-smi",
        "If reset fails (especially for Xid 79), system reboot may be required",
        "Monitor for XID recurrence after reset"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Reset Procedures",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Investigate Root Cause",
      "description": "Systematically investigate potential root causes including driver issues, PCIe problems, power issues, or hardware defects.",
      "objectives": [
        "Check driver version and age",
        "Review PCIe link status",
        "Check power and thermal conditions",
        "Look for error patterns"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=driver_version,pci.link.gen.current,pci.link.width.current,temperature.gpu,power.draw --format=csv",
        "lspci -vv | grep -i nvidia"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must investigate root cause",
          "expectedCommands": ["nvidia-smi --query-gpu", "lspci -vv"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Check driver version - outdated drivers can cause XIDs",
        "Verify PCIe link: Gen4 x16 for H100, Gen3 x16 for older GPUs",
        "Check 'lspci -vv' for PCIe errors (CorrErr, UncorrErr)",
        "Look for thermal throttling: temps >90C problematic",
        "Check power: GPUs require sufficient PSU capacity",
        "Xid 79 often caused by: PCIe slot issues, power instability, GPU hardware failure",
        "Review BMC logs for power events or hardware alerts"
      ],
      "estimatedDuration": 12,
      "documentationLinks": [
        {
          "title": "XID Troubleshooting Guide",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Check for Hardware Defects",
      "description": "Run diagnostics to determine if the GPU has a hardware defect requiring RMA.",
      "objectives": [
        "Run DCGM diagnostics",
        "Check ECC error counts",
        "Review historical error patterns",
        "Make RMA decision"
      ],
      "expectedCommands": [
        "dcgmi diag -r 3 -i 0",
        "nvidia-smi -q -i 0 | grep -i ecc"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run diagnostics",
          "expectedCommands": ["dcgmi diag", "nvidia-smi -q"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Run Level 3 diagnostics on affected GPU only",
        "If diagnostics fail, GPU likely has hardware defect",
        "Check ECC error counts: High uncorrectable errors indicate failure",
        "Recurring XIDs despite reset/reboot indicate hardware issue",
        "Document: XID type, frequency, conditions, diagnostic results",
        "Contact NVIDIA support if hardware RMA needed",
        "Use warranty lookup tool with GPU serial number"
      ],
      "estimatedDuration": 15,
      "documentationLinks": [
        {
          "title": "Hardware RMA Process",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Implement Preventive Measures",
      "description": "Take steps to prevent future XID errors based on the root cause analysis.",
      "objectives": [
        "Update drivers/firmware if needed",
        "Improve cooling if thermal issues",
        "Mark GPU for monitoring",
        "Document incident"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=driver_version --format=csv",
        "nvidia-smi -pm 1"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must implement preventive measures",
          "expectedCommands": ["nvidia-smi --query-gpu", "nvidia-smi -pm"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Update to latest recommended driver version",
        "Enable persistence mode to reduce driver unload/reload",
        "Improve airflow if thermal issues found",
        "Set up monitoring/alerting for GPU errors",
        "Document incident in maintenance log",
        "Schedule follow-up diagnostics",
        "If Xid recurs, remove GPU from production"
      ],
      "estimatedDuration": 9,
      "documentationLinks": [
        {
          "title": "GPU Maintenance Best Practices",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Successfully identified XID error in logs",
    "Interpreted XID error code and understood severity",
    "Checked GPU status after error",
    "Attempted GPU reset (if appropriate)",
    "Investigated root cause systematically",
    "Ran diagnostics to check for hardware defects",
    "Implemented preventive measures"
  ],
  "estimatedTime": 65,
  "prerequisites": ["domain1-driver-install", "domain4-dcgmi-diag"],
  "tags": [
    "xid-errors",
    "troubleshooting",
    "gpu-errors",
    "hardware-failures",
    "advanced"
  ],
  "tier": 3,
  "commandFamilies": ["gpu-monitoring", "diagnostics"],
  "explanationGateId": "gate-domain5-xid-errors",
  "toolHints": false
}
