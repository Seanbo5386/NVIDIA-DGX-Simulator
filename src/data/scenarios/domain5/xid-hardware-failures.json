{
  "id": "domain5-xid-hardware",
  "title": "Hardware XID Error Response: 27, 43, 54, 79",
  "domain": "domain5",
  "difficulty": "advanced",
  "description": "Master troubleshooting critical hardware-related XID errors for the NCP-AII certification. This scenario covers XID 27 (memory interface), XID 43 (GPU hung), XID 54 (hardware watchdog), and XID 79 (fallen off bus). These errors indicate severe hardware failures requiring systematic diagnosis.",
  "learningObjectives": [
    "Recognize hardware failure XID patterns",
    "Execute systematic hardware diagnostics",
    "Understand recovery vs replacement decisions",
    "Use thermal and power monitoring tools",
    "Document hardware failures for RMA"
  ],
  "faults": [
    {
      "nodeId": "dgx-03",
      "gpuId": 5,
      "type": "xid-error",
      "severity": "critical",
      "parameters": {
        "xidCode": 54,
        "description": "Hardware watchdog timeout"
      }
    }
  ],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Identify Hardware XID Errors",
      "description": "Hardware failures manifest as specific XID codes. Learn to identify XID 27, 43, 54, and 79 in system logs.",
      "objectives": [
        "Search for hardware XID errors in dmesg",
        "Identify the specific error code",
        "Correlate with affected GPU"
      ],
      "expectedCommands": [
        "dmesg | grep -i xid",
        "dmesg -T | grep -E 'Xid.*[2457][3479]'",
        "journalctl -k -p err | grep -i nvrm"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must search for hardware XID errors",
          "expectedCommands": ["dmesg | grep", "journalctl"]
        }
      ],
      "hints": [
        "XID 27: Memory interface error - VRAM communication failed",
        "XID 43: GPU hung - stopped responding to driver",
        "XID 54: Hardware watchdog timeout - internal health check failed",
        "XID 79: Fallen off bus - GPU completely disconnected",
        "Severity order: 79 > 54 > 27 > 43",
        "Multiple XID types = cascading failure"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "XID Error Reference",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Check GPU Accessibility",
      "description": "Before detailed diagnostics, verify if the GPU is still accessible to the system.",
      "objectives": [
        "Check nvidia-smi output",
        "Verify GPU is visible in lspci",
        "Determine if reset is possible"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "lspci | grep -i nvidia",
        "nvidia-smi -q -i 5"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check GPU accessibility",
          "expectedCommands": ["nvidia-smi", "lspci | grep"]
        }
      ],
      "hints": [
        "XID 79: nvidia-smi shows 'ERR!' or hangs completely",
        "XID 54: GPU may show but be unresponsive",
        "XID 43: Often recoverable with GPU reset",
        "XID 27: GPU visible but memory operations fail",
        "If lspci doesn't show GPU = PCIe-level failure",
        "'Unable to determine' = driver lost contact"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Recovery",
          "url": "https://docs.nvidia.com/deploy/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "XID 43: GPU Hung Recovery",
      "description": "XID 43 indicates the GPU stopped responding. This is often recoverable with proper intervention.",
      "objectives": [
        "Check if thermal throttling caused the hang",
        "Attempt GPU reset",
        "Verify recovery"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d TEMPERATURE",
        "nvidia-smi --gpu-reset -i 5",
        "nvidia-smi"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must investigate XID 43",
          "expectedCommands": [
            "nvidia-smi -q -d TEMPERATURE",
            "nvidia-smi --gpu-reset"
          ]
        }
      ],
      "hints": [
        "XID 43 CAUSES: Thermal, driver bug, hardware fault",
        "FIRST: Check temperature history",
        "High temp (>83C) = thermal throttling likely cause",
        "GPU reset: 'nvidia-smi --gpu-reset -i <gpu_id>'",
        "Reset may take 10-30 seconds",
        "If reset fails repeatedly: escalate to XID 54 procedures"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Reset Guide",
          "url": "https://docs.nvidia.com/deploy/gpu-reset/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "XID 27: Memory Interface Investigation",
      "description": "XID 27 indicates memory interface failure - communication between GPU core and VRAM is compromised.",
      "objectives": [
        "Check ECC error counters",
        "Run memory diagnostics",
        "Determine if memory controller failed"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d ECC",
        "nvidia-smi -q -d MEMORY",
        "dcgmi diag -r 3 -i 5"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must investigate XID 27",
          "expectedCommands": ["nvidia-smi -q -d ECC", "dcgmi diag"]
        }
      ],
      "hints": [
        "XID 27 = MEMORY CONTROLLER FAILURE",
        "Different from ECC errors (data corruption)",
        "Memory interface = physical bus to VRAM",
        "Check for thermal stress on memory chips",
        "DCGM diagnostic level 3 tests memory thoroughly",
        "Usually indicates GPU REPLACEMENT needed"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "GPU Memory Diagnostics",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "XID 54: Hardware Watchdog Analysis",
      "description": "XID 54 indicates the GPU's internal hardware watchdog detected a timeout - the GPU failed its own health check.",
      "objectives": [
        "Check system event log for power/thermal events",
        "Verify power delivery",
        "Determine if node reboot required"
      ],
      "expectedCommands": [
        "ipmitool sel list",
        "ipmitool sensor list | grep -i gpu",
        "ipmitool dcmi power reading"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must investigate XID 54",
          "expectedCommands": ["ipmitool sel", "ipmitool sensor"]
        }
      ],
      "hints": [
        "XID 54 = GPU FAILED INTERNAL HEALTH CHECK",
        "More severe than XID 43 (external timeout)",
        "CAUSES: Power glitch, thermal shutdown, hardware defect",
        "Check SEL for power events around error time",
        "GPU reset will NOT work for XID 54",
        "Node REBOOT required, then investigate"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "IPMI Reference",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "XID 79: Fallen Off Bus Recovery",
      "description": "XID 79 is the most severe - the GPU has completely disconnected from the PCIe bus and is unreachable.",
      "objectives": [
        "Confirm GPU is unreachable",
        "Check for thermal/power events",
        "Understand that reset is impossible"
      ],
      "expectedCommands": [
        "dmesg | grep -i 'fallen off'",
        "lspci | grep -i nvidia",
        "ipmitool sel list | tail -20"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must investigate XID 79",
          "expectedCommands": ["dmesg | grep", "lspci", "ipmitool sel"]
        }
      ],
      "hints": [
        "XID 79 = GPU COMPLETELY OFFLINE",
        "nvidia-smi will hang or show no GPUs",
        "GPU reset IMPOSSIBLE - device unreachable",
        "CAUSES: Power failure, thermal shutdown, PCIe failure",
        "IMMEDIATE ACTION: Node reboot required",
        "If recurs after reboot: Hardware replacement needed"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Recovery Procedures",
          "url": "https://docs.nvidia.com/deploy/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Post-Reboot Verification and RMA",
      "description": "After recovering from hardware errors, verify system health and prepare RMA documentation if needed.",
      "objectives": [
        "Verify all GPUs after reboot",
        "Run diagnostics to confirm health",
        "Collect RMA documentation if recurring"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "dcgmi diag -r 2",
        "nvidia-bug-report.sh"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify and document",
          "expectedCommands": ["nvidia-smi", "dcgmi diag", "nvidia-bug-report"]
        }
      ],
      "hints": [
        "After reboot: All 8 GPUs should be visible",
        "Run dcgmi diag -r 2 for quick health check",
        "Single occurrence: Monitor closely",
        "Recurring errors: GPU REPLACEMENT required",
        "Document: Error timeline, GPU UUID, SEL logs",
        "For DGX: Check cooling system, power rails"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NVIDIA Support",
          "url": "https://www.nvidia.com/en-us/support/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Identified hardware XID error codes in logs",
    "Verified GPU accessibility after errors",
    "Attempted XID 43 GPU hung recovery",
    "Investigated XID 27 memory interface issues",
    "Analyzed XID 54 hardware watchdog failures",
    "Handled XID 79 fallen off bus situation",
    "Created proper RMA documentation"
  ],
  "estimatedTime": 50,
  "prerequisites": ["domain5-critical-xid"],
  "tags": [
    "xid-27",
    "xid-43",
    "xid-54",
    "xid-79",
    "hardware-failure",
    "troubleshooting",
    "advanced",
    "exam-critical",
    "rma"
  ],
  "tier": 3,
  "commandFamilies": ["gpu-monitoring", "bmc-hardware", "diagnostics"],
  "explanationGateId": "gate-domain5-xid-hardware",
  "toolHints": false
}
